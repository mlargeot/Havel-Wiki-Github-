# Functional Specifications

## Summary

- [1. Project Overview](#1.-project-overview)
- [2. Context and Vision](#2.-context-and-vision)
- [3. Functional Objectives](#3.-functional-objectives)
- [4. Technical Objectives](#4.-technical-objectives)
- [5. Complementary Objectives](#5.-complementary-objectives)
	- [5.1. Collaboration with Experts](#5.1.-collaboration-with-experts)
	- [5.2. Performance Optimization](#5.2.-performance-optimization)
- [6. Expected Outcomes](#6.-expected-outcomes)
- [7. Long Term Vision](#7.-long-term-vision)
- [8. Summary of Alignment with Epitech Objectives](#8.-summary-of-alignment-with-epitech-objectives)
- [9. Conclusion](#9.-conclusion)

## 1. Project Overview

**Project name:** HAVEL  
**Type:** Epitech Innovative Project (EIP) — Technical Track  
**Side Objectives:** *Performance & Expert Collaboration*  
**Team Objective:** Design and implement a performant, scalable, and intuitive **CTF (Capture The Flag) management platform** capable of hosting and orchestrating both simple and complex cybersecurity challenges.

---

## 2. Context and Vision

The cybersecurity community regularly organizes **CTF events** that require deploying isolated, challenge-specific environments.  
Existing solutions (e.g., CTFd) often lack:
- fine-grained control over virtualized infrastructures,  
- optimized performance during event scaling,  
- and modular integrations with orchestration or network tools (e.g., Docker, Kubernetes, Proxmox, WireGuard).

**Havel** aims to fill this gap by offering:
- an **intuitive interface** for managing challenges and events,  
- a **powerful orchestration system** supporting containers and VMs,  
- and a **modular backend** designed for performance and security.

The platform’s final goal is to become a **complete and extensible CTF infrastructure management solution**, suitable for both small local events and large-scale competitions.

---

## 3. Functional Objectives

| Category                      | Description                                                                                                                                                              |
| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Challenge Management**      | Create, modify, and delete challenges; manage metadata, flags, scoring, and challenge types (Q&A, containerized, VM-based, network-based).                               |
| **Event Management**          | Manage event lifecycle (creation, setup, customization, administration); user and team management; permissions and roles.                                                |
| **Hosting and Orchestration** | Deploy and manage Docker containers or virtual machines per challenge. Future versions will extend this to full network topologies (via Proxmox, Kubernetes, WireGuard). |
| **User Interface**            | Provide a clear and accessible interface for administrators and participants; real-time updates; consistent UX.                                                          |
| **Performance & Security**    | Ensure optimized deployment times, scalable resource handling, and secure isolation of challenge environments.                                                           |

---

## 4. Technical Objectives

#### Mandatory Requirements

As part of the *Technical Track*, Havel must demonstrate **technical mastery and architectural rigor** through the following points:

| Requirement | Deliverable |
|--------------|-------------|
| **Active technology watch** | Continuous research and documentation of orchestration, virtualization, and networking technologies (Docker, Proxmox, Kubernetes, WireGuard…). |
| **Proofs of Concept (POC)** | One or more POCs demonstrating successful integration or benchmarking of a new technology. |
| **Benchmarking & experimentation** | Comparative performance tests (deployment time, scalability, resource footprint). |
| **Integration of new technology** | Incorporation of at least one technology tested and validated through experimentation. |
| **Documented architecture** | Architecture diagram, module breakdown, service interconnections, and infrastructure overview. |
| **Complete README** | Installation, environment setup, and configuration instructions. |
| **Code standards & testing** | Application of linting, naming conventions, unit/integration testing, and security validation (authentication, data protection). |

---

## 5. Complementary Objectives

The team has selected the following **two complementary objectives** for the Technical Track:

### 5.1. Collaboration with Experts
- Identify relevant experts (DevOps, cybersecurity engineers, network architects, ctf event managers).  
- Organize structured exchanges (interviews, code reviews, architecture validation).  
- Apply concrete improvements from feedback (deployment optimization, security hardening, design adjustment).  
- Maintain a traceable collaboration log (contacts, advice, implemented changes).

**Deliverables:**
- Expert contact sheets and proof of exchange (email, meeting, GitHub thread).  
- Summary notes and impact documentation.

---

### 5.2. Performance Optimization
- Define relevant metrics (deployment time, response latency, resource load).  
- Execute load, stress, and resilience tests.  
- Analyze bottlenecks and implement optimizations (asynchronous tasks, caching, lightweight virtualization).  
- Document before/after results and justify trade-offs.

**Deliverables:**
- Chosen indicators and monitoring dashboard (logs, charts).  
- Optimization plan and comparative performance reports.  
- Technical rationale for applied improvements.

---

## 6. Expected Outcomes

| Phase | Deliverables |
|-------|---------------|
| **Phase 1 – Architecture Definition** | System architecture documentation, technology watch notes, POC reports. |
| **Phase 2 – Implementation** | Functional modules for challenge & event management, performance test integration, expert review session. |
| **Phase 3 – Optimization & Validation** | Performance benchmarking results, updated documentation, final expert validation, complete technical README. |

---

## 7. Long Term Vision

The project will evolve toward **Havel v1.0**, capable of:
- Deploying both containers and full virtual networks,
- Managing large-scale multi-event CTF competitions,
- Integrating real-time monitoring and scoring dashboards,
- Supporting plug-ins for automatic challenge provisioning,
- And serving as a base for further open-source contributions.

---

## 8. Summary of Alignment with Epitech Objectives

| Epitech Objective | Status in Havel |
|--------------------|----------------|
| **Veille technologique active** | Continuous benchmarking of virtualization/orchestration tools. |
| **POC / expérimentation** | Integration tests of Proxmox, Docker, and WireGuard. |
| **Architecture documentée** | Dedicated architecture section with diagrams and justifications. |
| **Standards & sécurité** | Code standards defined; testing and authentication included. |
| **Objectif complémentaire : Performance** | Measured and optimized through benchmarks and load tests. |
| **Objectif complémentaire : Collaboration avec experts** | Validated via structured expert exchanges and applied improvements. |

---

## 9. Conclusion

The **Havel** project stands as a technically ambitious EIP aiming to redefine how CTF infrastructures are deployed and managed.  
By combining **performance engineering**, **technical collaboration**, and **rigorous documentation**, it perfectly embodies the expectations of the Epitech Technical Track — both in depth of execution and in professional maturity.
